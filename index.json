[{"categories":["AI"],"content":"Chapter 1-But what is a Neural Network 以识别数字的例子进行引入，人脑可以将看到的不同形状写法的数字分辨成0-9的某一个，但是如何让机器程序来解决？ 将这个问题描述为：编写一个程序，输入是一个28×28像素的网格，输出0到9之间的某个数字。 这个对于人眼和大脑都非常容易的问题，对于程序却不好描述，比如传统的if语句、类、对象等等。神经网络则试图编写一个模仿大脑的程序来解决这些问题。 ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:1:0","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"神经网络的结构 神经网络多种多样，例如图像处理中广泛使用的CNN，还有RNN和transformer等。我们从最简单最经典的模型来考虑，即多层感知机Plain vanilla(aka “multilayer perceptron”) 神经元 首先将神经元考虑为一个“装有数字的容器”，0.0-1.0之间的值表示被激活的“程度”，来类比人脑中神经元的活跃程度。一堆连接在一起的神经元就构成了神经网络。 对于本问题，输入图像中有28 × 28 = 784个像素，每一个的亮度值都介于0.0和1.0之间，因此我们在网络中对应设计一个有784个神经元的 “输入层” ，同样的最后一层的 “输出层” 包含10个神经元，代表0-9中某个可能的数字。从图中可以看出，在中间还会有一些层，即 “隐含层” ，涉及到一些处理，在图中的网络中设计了2个隐含层，每一层有16个神经元，设置可以是任意的，可以根据情况选择层数和神经元数。 层(Layers) 为什么需要Layers？可以从图中看到某一层的每个神经元会通过一条线连接到下一层的每一个神经元，通常这些线是带有权重的，表示某层中每一个神经元的\"activation\"(激活)如何对下一层的每个神经元的激活产生影响。而确定这些权重的选择，是神经网络作为信息处理机制。我们需要考虑几个问题：为什么期望这样的分层结构能够智能运行？我们期待什么？中间层做的是什么？为什么不直接把所有像素连接到最终输出？ 先从一个 不一定正确 的想法谈起，我们可以希望倒数第二层的每个神经元识别出组成数字的更小的、可识别的组件，比如圆圈，长线段等等，并希望第二层可以对应一些更小的组分，如下图所示： 虽然事实可能与我们这种猜想不同，但我们可以推断到的是，层将问题分解为一些很小的“碎片”。除了图像处理外，解析语音也会用到分层，例如先讲原始音频解析为不同的声音，然后组合成某些音节，再结合成单词，进一步构成短语以及更想法等。因此，神经网络的分层结构是很棒的，它允许我们将困难的问题分为bite-size steps，因此从一层移动到下一层相对会简单一些。 ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:1:1","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"信息在层之间的传递 在神经元之间的连线上分配权重，代表着该层的神经元与下一层的新神经元如何相关。如果该层的某个神经元是激活的，正权重代表着与之相连的下一层神经元也应该是激活的，负权重则相反。各个权重会以一种有趣的方式相互作用和冲突。得到第二层的神经元的值需要计算第一层所有神经元激活值的加权和。我们自然可以想到如果我们想找某一块中是否有某一个小“碎片”，不妨将目标区域对应的权重都设为正值，其它赋值为零，这样的确在目标区域有我们希望的“碎片”时更容易将下一层的神经元激活，但是设想一种输入像素全都是激活(纯白)的图像，这种方法的计算结果仍然是“激活”，所以我们如果真的想确定这存在着一条短边(“碎片”)，需要给目标区域周围一圈的权重赋值为负，这样当目标区域像素亮而周围像素暗的时候，综合将最大，例如对于下图的例子中，激活程度B\u003eD\u003eA\u003eC. Sigmoid Squishification \u0026 Bias 上述加权和可能是任何一个数，但是对于该网络我们希望激活值处于0.0-1.0之间，所以需要对得到的结果进行“压缩”。sigmoid是其中一个常用的函数。 但我们希望的可能并非是在大于零时将神经元激活，可能希望加权和大于某个设定值再让其激活，所以我们可以引入Bias，计算公式为： $$ \\sigma(\\omega_{1}a_{1} + \\omega_{2}a_{2} + … +\\omega_{n}a_{n} + Bias) $$ 可以得出，权重告诉我们第二层的神经元关注什么样的pixel pattern，而Bias告诉我们加权和需要多大才认为神经元的激活有意义。 注：sigmoid是有很明显的缺点的。从图中可以看出在数变得极大或者极小时这个函数的变化非常平坦，这不利于我们训练网络。因为当我们调整权重的时候，可能根本没有明显的变化。所以当前广泛采用ReLU(Rectified Linear Unit)。它的输出永远不会变平缓，因此调整权重总能够提供有效的反馈，使得训练过程更快更高效，尤其是设计的层数很多时(深度学习)。 我们采用sigmoid/ReLUd的原因是如果我们单纯的计算加权和，整个函数会是线性的，这会使得网络dramatically less expressive。 More neorons 刚刚是针对一个神经元讨论，但第一层就有784个神经元，则与第二层之间有784×16个权重和16个Bias，加上其它层，权重和偏差加起来一共有13002个，也就是说我们需要针对很大规模的参数进行调节。在之前提到我们对于网络工作模式的设想不一定是正确的，但比起将隐含层当成一个黑箱，这样通过设想来猜测意义，可以对我们修正结构有方向性的帮助。理想情况下网络既能够正确工作，我们也能知道结构设计和参数的缘由。可以通过一些简化的记号通过矩阵的写法来表示众多的参数，这也利于编程： ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:1:2","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"网络只是一个函数 之前提到将神经元看做保存数字的容器，其实将其视为一个函数更加准确，它的输入是上一层神经元的所有输出，它的输出是一个0-1之间的值，而图示的整个网络，可以看做输入784的值，输出10个值的函数。 ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:1:3","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"Chapter2 Gradient descent, how neural networks learn 梯度下降法是训练的核心思想，不仅是神经网络学习的基础，也是其它许多机器学习技术的基础。 ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:2:0","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"网络如何学习 机器学习与其它计算机科学的不同之处在于，我们没有为执行特定任务编写指令。对于第一节的例子，我们没有编写识别数字的算法，相反我们编写了一个可以接收一堆手写数字的示例图像以及它们是什么的标签的算法，并且调整网络的13002个参数(输入的标记图像称为“训练数据”)。我们通过这些数据来训练网络，然后使用一些网络没见过的数据在训练后对其进行测试。 机器的学习过程更像是一个微积分的问题，它目的是找到特定函数的最小值。 成本函数 现在很多时候将cost/lost function混为一谈。回顾一下，加权值和中的权重可以认为代表连接的强弱，而偏置值则表明神经押元是否更容易被激活。首先将所有权重和偏差初始化为随机数，网络会表现很糟，需要设计一种方式来识别目前做的很糟糕，然后帮助它改进。所以我们来定义一个成本函数，将输出与期望值之间的差异平方相加，称为该训练数据的“成本”。 The Cost Over Many Examples 为了真正衡量网络的性能，我们要考虑所有(可能数万个)训练示例的平均成本，这个平均值是我们衡量网络有多糟糕以及计算机应该感觉有多糟糕的指标。同样的，这个平均成本也是一个复杂的函数，网络本身就是一个复杂的函数，平均成本(代价函数)要再网络的基础上进行更高一层的计算，它将13002个权重和偏置值作为其参数，而输出一个数值(差的平方和)。 Minimizing the cost function 告诉计算机有多么糟糕只是一部分，我们更需要告诉它应该如何改进。对于简单的函数，我们可以通过求导，来知道在什么位置可以取到(局部)最小值，但对于复杂的函数，本次实例中上万个参数的函数，是无法这样进行的。一个更灵活的技巧是，先随便挑一个输入值，然后考虑往什么方向走，可以将函数值减小，这样在每一个点都求梯度，并沿着下降的方向走一小步(斜率越小这个步长就越小，从而防止调整过度)，就可以逼近函数的某个局部最小值(可能存在很多，取决于从哪个随机输入开始)，可以想象成小球从山上往下滚。 梯度 在高维空间中我们需要一个向量来表示最陡峭的方向，多元微积分中将这个向量称为”梯度“，代表哪个方向增加函数最快，取该向量的反方向就可以得到最快减小函数的步进方向，这个梯度向量的长度表示了这个最陡的斜坡有多陡。所以最小化这个函数的算法是计算这个梯度方向，然后下降，不断重复，这个过程就叫做”梯度下降“。 如果我们把这13002个权重和偏置值都放到一个列向量里，则代价函数的(负)梯度也会是一个有13002个条目的向量。 负梯度指出了我们应该如何调整每个参数，才可以让代价函数的值下降的最快。该向量的每个分量告诉我们两件事，符号表示相应分量应该向上还是向下，更重要的一点是，梯度中各个分量的相对大小告诉我们这些调整哪一个更为重要(改变哪个性价比更高)。计算梯度的算法是神经网络的核心，称为back propagation(反向传播算法)。 注意，我们一直提到寻找最小值，这要求代价函数必须是平滑的，这也解释了为什么人工神经元的激活值是连续的，而不是二元式。 回顾一下我们之前设想的，第二层识别一些小的“碎片”，第三层识别更大一些的pattern，但事实上这个网络本身并不是这样做的，第二层的结果看起来会杂乱无章，只是代表某个局部最小值而已。而且因为我们的训练数据只有从0-9的标签数据，相当于整个空间只局限于此，所以训练好的网络也可能将某个毫无意义的噪声图像识别为某个数字。当然MLP只是一个很古老的模型，现在有更多更好用的新模型出现。 ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:2:1","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"Chapter 3-Analyzing our neural network 这是一个mini lesson。将分析训练好的网络做了什么以及原因。之前例子中的网络训练好后大概有96%的正确率，这其实是难以置信的，因为我们从来没有明确告诉它要寻找怎么样的pattern。 之前我们猜测第二层识别小的边缘，第三层识别更大一些的子组件来激发这个结构。但事实上我们的网络不是这样的，查看从第一层到下一层的过度相关联的权重时，它们看起来几乎是随机的，虽然有一些松散的pattern，但显然不是我们之前设想的： 似乎在可能权重和偏差的13002维空间中，网络发现了某一个局部最小值，尽管成功的对大多数图像进行了分类，但并没有完全掌握更普遍的模式，例如之前提到的随机输入一个噪声图像，智能的系统应该输出不确定的结果(要么没有激活，要么均匀激活输出层)，但其实它自信的给出了一些无意义的答案。而且这个网络的训练数据基本都是居中且大小合适的数字，并没有让网络在网格的一个区域上获取的模式知识转移到另一个区域，我们的训练算法甚至没有使用某些像素和其他像素相邻的事实。 ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:3:0","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"Chapter4-What is backpropagation really doing ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:4:0","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"The Intuition for Backpropagation 反向传播中的各种符号和上下标一开始会使人困扰。首先我们忽略这些公式符号，从逐步了解每个训练示例对权重和偏差的影响开始。 对于这个样本，我们希望网络分类为2，我们希望输出值的变动大小应该与当前值和目标值之间的差异成正比，例如，增加数字“2”神经元的激活值要比减少数字“8”的激活值更为重要，因为后者已经接近理想输出。我们以“2”神经元为例： 它的激活值是来自于前一层所有的加权综合，加上一个偏置值，最后经过sigmoid/ReLU等进行处理。因此我们有三种方法来增大这个激活值： 增加偏置值 增加权重 改变上一层的激活值 增加偏置值 这是最简单的方法，与改变前一层的权重或者激活值不同，改变偏置对加权和的影响是恒定而且可预测的。 增加权重 权重与激活值相乘，所以具有不同程度的影响，与来自前一层更亮的神经元的连接具有较大的影响，因为这些权重与更大的激活值相乘。即当我们进行梯度下降时，不仅关心每个是要向上还是向下调整，更关心调整哪些参数性价比最高。 这里类似于神经科学中关于神经元如何学习的理论——“赫布理论(Hebbian theory)”，即“一同激活的神经元关联在一起”。这里，权重的最大增长，即最大的连接加强的部分，就会发生在已经最活跃的神经元和我们希望变得更活跃的神经元之间。 改变前一层的激活值 如果前一层中所有正权重连接的神经元更亮而负权重连接的神经元更暗的话，就可以增加我们想要的神经元的激发程度。与改变权重的时候类似，我们想造成更大的影响，就要根据对应权重的大小，对激活值做出成比例的改变。当然，我们无法直接改变激活值。 ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:4:1","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"对所有的训练数据重复 我们之前只考虑了要增强2的激活，但还要减弱最后一层中其他神经元的激活。但每个神经元对于如何改变倒数第二层都有自己的想法，这意味着有许多竞争性的请求来改变前一层。所以我们会将数字“2”神经元的期待和其它输出神经元的期待全加在一起，作为如何改变倒数第二层神经元的指示。这些期待的变化不仅与对应的权重成比例，也与每个神经元激活值需要改变的量成比例。这其实就是“反向传播”的理念，我们可以一直递归的将这个过程进行从后一层到前一层，直到第一层。 刚刚我们讨论的是仅仅对一个训练样本进行的过程，我们还需要对其他所有的训练样本同样的进行一遍反向传播。记录下每个样本想要怎样修改，然后取平均值。 这里一系列的对每个权重和偏置的平均微调大小，不严格的讲，就是之前说的代价函数的负梯度(至少是其标量的倍数)。 ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:4:2","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"Stochastic Gradient Descent 实际操作中如果梯度下降的每一步都用上每一个训练样本来计算会非常耗时。可以采取一个技巧，首先将训练数据打乱并分成很多组minibatch。然后根据每个minibatch而不是整个训练样本集来计算梯度下降。它并不会提供准确最有效的梯度下降，但给出了一个很好的近似值，而且计算时间大大减少，这就是随机梯度下降。我们并不是小心谨慎的一步步沿着最佳方向走到底部，而是有点像个醉汉快速但是绕着小圈子跑下去。 注：我们需要非常多的标记好的数据才能良好的完成训练过程。 ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:4:3","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"Chapter5-Backpropagation calculus 从微积分的角度看在机器学习中我们一般是怎么理解链式法则的。 梯度向量的条目是代价函数相对于网络所有不同权重的偏置的偏导数，也就是说反向传播帮我们计算出了那些导数。 ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:5:0","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"用反向传播计算梯度 首先我们设定一些记号： a(L) 表示L层某个神经元的激活值，y是我们期待的输出，C0是代价函数，我们想知道在数轴上挪动ω可以对C造成多大影响，就是求导，可以利用链式法则。 网络的完整代价函数是求的每个样本的平均值，所以导数我们也要求平均值。而这个只是梯度向量的一个分量，所有权重和偏置的导数组成梯度向量。从求导结果我们可以看出，当前一个神经元更活跃时(a(L-1))，改变ω可以有更好的效果，偏导的最后一部分告诉我们该导数与实际输出与期望输出之间的差异成正比，也就是说当实际输出偏差很大时，即使微小的改动也会对代价产生明显的影响。 偏置的梯度更简单一些： ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:5:1","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"前一层的权重和偏置 我们已经确定了最后一层的权重和偏置的变化如何影响总体的成本，这意味着我们已经有了梯度向量的两个部分(前面提到三种改变方式)。现在我们要探讨一下代价函数对上一层激活值的敏感程度(对a(L-1)求偏导)。虽然我们不能直接改变激活值，但我们仍应该关注这一方面，因为我们可以反向利用链式法则，来计算代价函数对之前(前一层)的权重和偏置的敏感度。 例如求偏导$\\frac{\\partial(C_0)}{\\partial(w^{(L-1)})}$​​​可以通过如下的链式法则得出，因此我们可以通过链式法则得到任何一个参数的导数，即可以计算出整个梯度向量的各个分量。 $$ \\frac{\\partial(C_0)}{\\partial(w^{(L-1)})}=\\frac{\\partial(z^{(L-1)})}{\\partial(w^{(L-1)})}\\frac{\\partial(a^{(L-1)})}{\\partial(z^{(L-1)})}\\frac{\\partial(z^{L})}{\\partial(a^{(L-1)})}\\frac{\\partial(a^{L})}{\\partial(z^{L})}\\frac{\\partial(C_0)}{\\partial(a^{L})} $$ ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:5:2","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["AI"],"content":"更复杂的网络 上面的例子中是“一条线”的简单网络，对于复杂网络其实没有太大变化，只是多了一些下标而已，来指出这是该层的哪个神经元对应的量。而代价函数和对于上一层激活值的偏导变成了求和的模式： 可以用下面的公式总结，对于下面黄色框的公式，当不是最后一层的时候用上式，是最后一层用下式。 ","date":"2021-08-14","objectID":"https://maxwellf1.github.io/2021-08-14/:5:3","series":null,"tags":["3Blue1Brown","Neural Networks","AI"],"title":"3Blue1Brown-Neural Networks","uri":"https://maxwellf1.github.io/2021-08-14/"},{"categories":["Tools"],"content":"1. 连接 通过SSH，首先需要确定自己已经安装了openSSH，本次是在Windows环境下进行。选用自己习惯的软件即可，我用过putty, Xshell, VScode等等，由于自己太懒配置不好vim，而且终端看久了眼疼，就想采用VS Code来进行连接。 ","date":"2021-08-12","objectID":"https://maxwellf1.github.io/2021-08-12/:1:0","series":null,"tags":["Server"],"title":"远程服务器连接","uri":"https://maxwellf1.github.io/2021-08-12/"},{"categories":["Tools"],"content":"通用流程 安装SSH和VS code，并在VS code的扩展商店里添加Remote SSH相关插件。 生成密钥。在.ssh目录下可以用ssh-keygen命令生成公钥和私钥，并将公钥传到远程服务器的根目录下.ssh文件夹中。 连接远程主机(例如通过ssh username@ip -p port命令)，在.ssh目录下通过cat xxx.pub \u003e\u003e authorized_keys命令将公钥添加到authorized_keys中。 采用私钥进行登录(例如ssh username@ip -p port -i id_rsa命令)。点击VS code中Remote SSH的设置键，通常选第一个configuration file进入，根据需要填写下面的参数信息： Host: 连接的主机的名称，可以自己起名 Hostname: remote server的IP地址 Port: 用于登录远程服务器的端口 User: 用于登录远程主机的用户名 IdentityFile: 私钥在本地的路径 ProxyCommand: 需要执行的指令 完成并保存后就可以发现在左侧添加上的新的远程服务器，点击连接即可。 ","date":"2021-08-12","objectID":"https://maxwellf1.github.io/2021-08-12/:1:1","series":null,"tags":["Server"],"title":"远程服务器连接","uri":"https://maxwellf1.github.io/2021-08-12/"},{"categories":["Tools"],"content":"跳板机 有时候例如自己实验的的服务器是设置在内网环境中的，用自己的客户端在公共环境无法直接连接，通常采用跳板机来解决这个问题。一般是先通过ssh连接到跳板机之后，再跳转到所需的服务器，同样是只能用vim使用命令行环境，由于配置起来比较麻烦，使用不便，还是想用VS code来提高便捷性。配置的过程与通用流程稍有不同，因为涉及到两个机器，所以在configuration files中要按照如下方式填写： Host JumpMachine #跳板机名称 HostName XXX.XXX.XXX.XXX #跳板机IP Port XXX #跳板机ssh端口 User root #跳板机用户名 IdentityFile xx\\xx\\xx.. Host TargetMachine #远程服务器名称 HostName XXX.XXX.XXX.XXX #远程服务器IP Port XXX #远程服务器ssh端口 User root #远程服务器用户名 ProxyCommand ssh(替换为自己安装的ssh的路径，例如C:\\Windows\\System32\\OpenSSH\\ssh.exe) -W %h:%p JumpMachine 同样的，为了省去密码等，可以发送公钥到跳板机和服务器，并添加到authorized_keys中(并在参数中添加IdentityFile)。(注：所有前序机器的id_rsa.pub都要添加到最终机器上。比如说有3台机ABC，其中B是跳板机。那么A的.pub要在B跟C上分别导入一次，B的.pub要在C上导入一次，共3次，才能完全实现免密登录远程服务器) 为了防止发送公钥时覆盖了目标机器上的authorized_keys文件，可以用ssh-copy-id命令来复制公钥 ssh-copy-id -i id_rsa.pub \"-p 跳板机的ssh端口 用户名@跳板机IP\" 将笔记本的公钥也同样的发送到远程服务器中 可以先使用scp将公钥发送至跳板机 scp -P 跳板机ssh端口 id_rsa.pub 跳板机用户名@跳板机IP:~/.ssh/temp 再通过ssh连接至跳板机，并切换到~/.ssh目录下，将其发送至远程服务器 scp -P 远程服务器ssh端口 temp 远程服务器用户名@远程服务器IP:~/.ssh/temp 最后通过ssh连接至远程服务器，切换到~/.ssh目录下，手动将公钥拼接到authorized_keys中 cat temp \u003e\u003e authorized_keys 配置保存后就可以在左边的SSH TARGETS中看到配置好的JumpMachine和TargetMachine，选择TargetMachine进行连接即可。 ","date":"2021-08-12","objectID":"https://maxwellf1.github.io/2021-08-12/:1:2","series":null,"tags":["Server"],"title":"远程服务器连接","uri":"https://maxwellf1.github.io/2021-08-12/"},{"categories":["Tools"],"content":"2. 文件传输 个人由于懒而且记不住命令还是比较习惯有GUI，平时比较常用的是WinSCP,这是一个在Windows环境下使用的SSH的开源图形化SFTP客户端，同时支持SCP协议。它可以在本地与远程计算机之间安全的传输文件，并可以直接编辑文件。它可以很直观的看到文件的层级结构和控制文件的传输而无需依赖其它插件。 ","date":"2021-08-12","objectID":"https://maxwellf1.github.io/2021-08-12/:2:0","series":null,"tags":["Server"],"title":"远程服务器连接","uri":"https://maxwellf1.github.io/2021-08-12/"},{"categories":["Tools"],"content":"基本使用 新建会话主机名填入IP，端口号填入自己的端口号，再输入用户名和密码，保存并登陆之后应该可以看到文件的界面。 ","date":"2021-08-12","objectID":"https://maxwellf1.github.io/2021-08-12/:2:1","series":null,"tags":["Server"],"title":"远程服务器连接","uri":"https://maxwellf1.github.io/2021-08-12/"},{"categories":["Tools"],"content":"跳板机 在session界面点击高级选项，在高级设置中找到隧道/Tunnel，勾选上使用SSH隧道，并输入跳转机的IP、端口号以及用户名和密码，如果不输入密码，可以在Tunnel界面下方添加自己的私钥文件(需要是ppk，如果不是可以用PuTTYgen来生成/转换)。登录界面的信息填最终想通过跳转机连接到的服务器的配置即可。 ","date":"2021-08-12","objectID":"https://maxwellf1.github.io/2021-08-12/:2:2","series":null,"tags":["Server"],"title":"远程服务器连接","uri":"https://maxwellf1.github.io/2021-08-12/"},{"categories":["Tools"],"content":"命令 scp是secure copy的简写，用于在Linux下进行远程拷贝文件的命令，和它类似的命令有cp，不过cp只是在本机进行拷贝不能跨服务器，而且scp传输是加密的。可能会稍微影响一下速度。当你服务器硬盘变为只读 read only system时，用scp可以帮你把文件移出来。另外，scp还非常不占资源，不会提高多少系统负荷，在这一点上，rsync就远远不及它了。虽然 rsync比scp会快一点，但当小文件众多的情况下，rsync会导致硬盘I/O非常高，而scp基本不影响系统正常使用。 当在同一个服务器不同账户之间传输文件时可以用scp命令。例如想把服务器下B用户目录下的某文件b拷到A用户目录下，可以采用如下指令 scp -r dir_B/b serverIP:/dir_A 具体命令及参数可以见参考文献的几个链接。 ","date":"2021-08-12","objectID":"https://maxwellf1.github.io/2021-08-12/:2:3","series":null,"tags":["Server"],"title":"远程服务器连接","uri":"https://maxwellf1.github.io/2021-08-12/"},{"categories":["Tools"],"content":"参考文献 VS Code Remote SSH配置 vscode通过跳板机连接远程服务器_huitailangyz的博客-CSDN博客 使用WinScp连接远程服务器和传输文件 - 你要 - 博客园 (cnblogs.com) winscp通过跳板机访问远程服务器（使用秘钥的方式传输文件） - 华为云 (huaweicloud.com) Linux scp -r命令主机间文件复制_学亮编程手记-CSDN博客 scp详解_LZJWXJ树袋熊-CSDN博客 Linux在同一台服务器不同账号之间传输文件_蓝一潇的博客-CSDN博客 ","date":"2021-08-12","objectID":"https://maxwellf1.github.io/2021-08-12/:3:0","series":null,"tags":["Server"],"title":"远程服务器连接","uri":"https://maxwellf1.github.io/2021-08-12/"},{"categories":[],"content":"1.为什么建立这个博客 博客这玩意儿，应该很早以前就有了。记得我上小学的时候老师们通过某个叫“校讯通”的平台建立了博客，有时候会发布一些信息，记录一些班级事务之类的。当时我也跟风，没记错的话是在网易建立了自己的博客，还各种费心思换主题，有水墨、竹林、牛皮纸等(🤣。后来也就当成了一个玩具一样，改个配色，改个字体，也没有当成个正事儿，最后随着各种新兴事物的出现，早就把写博客这档子事儿抛到九霄云外去了。 我从来不喜欢动手记录，例如中学的时候老师一遍又一遍的反复教育我整理错题和笔记，我却感觉这是浪费时间，完全不弄或者随便糊弄几笔了事儿。大学时候。我仍然引以为傲的保持着自己的“优良传统”，其实逐渐感觉到吃力了，毕竟这些课程比中学那点知识不知道高到哪里去了，但为了保持自己可笑的“独特习惯”，我就是不愿意记笔记，作业也是写完就了事儿，逐渐出现了很多自己一次又一次记不住的知识，写不对的题，敲不出来的code。还带着之前的思维，毕竟中学总觉得自己挺聪明，效率高，而且现在自己这么混，都没有挂科，成绩还能凑合中等，“要是我努努力，细心一点，肯定能变得很厉害，我只是不愿意罢了”。其实那个时候，我开始发觉自己已经眼高手低了，只是心底里想逃避罢了，“要是我更用心，反而没什么提高，别人会不会笑话我效率低，笑话我是个笨蛋呢”，所以还是保持着陋习。2018年我还看了姜文的电影，《邪不压正》，开玩笑的跟哥们说道：“正经人谁记笔记啊？是啊。你记笔记吗？我不记，你记笔记吗？谁能把学会的写笔记里？写出来的能叫学会的？下贱！”👎更觉得记东西反而是浪费时间，会了就是会，不会的咋着都学不会🤡 后来，一些事情改变了我，我开始意识到自己的不足，也决定尝试改变，摆正自己的定位，做一些自己之前看不上眼的事儿来补救。但仍然觉得很低效，看似自己在忙活着，一段时间过后回想起来，感觉自己还是什么都没学会，再遇到同样的问题时，很多时候还是要重新查，一直没想到该怎么解决这个问题。最近看到网上的推荐，读了Philip J.Guo的The Ph.D. Grind，感触颇深，想不错的完成博士学业确实是一门学问，无论是身心还是自己的学术技能都要不断的经受Grind，但如何度过好这个训练的过程，让这些grind变得有意义，是读博应该思考的。其实这一年来我也在思考类似的问题，也找了很多长辈和同辈讨论，后来总结出 “反馈” 大概对于很多人至少对于我是很重要的。一方面我很畏惧没头没尾的不踏实感，有反馈才能更好的证明“存在”；另一方面，费曼学习法中通过自己的总结和他人的反馈，能够帮助自己更好的消化知识，或者触发新的灵感。 因此，我选择开始写博客来记录。这样回过头来可以大概知道自己学到或者曾经接触了些什么，还能在不断的回顾中加深印象，查漏补缺。我再也不想因为一些幼稚的想法而耽误自己，努力，永远不嫌早，也永远不会太晚。 ","date":"2021-08-11","objectID":"https://maxwellf1.github.io/2021-08-11/:1:0","series":null,"tags":[],"title":"写在开始的话","uri":"https://maxwellf1.github.io/2021-08-11/"},{"categories":[],"content":"2.一些说明 ","date":"2021-08-11","objectID":"https://maxwellf1.github.io/2021-08-11/:2:0","series":null,"tags":[],"title":"写在开始的话","uri":"https://maxwellf1.github.io/2021-08-11/"},{"categories":[],"content":"2.1关于本博客 本博客基于HUGO的DoIt主题创建。 Title的icon采用了earlybirds，以激励自己笨鸟先飞。 主页的subtitle\"觉悟者恒幸福\"是jojo的奇妙冒险第六部石之岛中恩里克普奇神父说的一句话，我赞同这句话，但我并不认为普奇神父自己做到了这一点，相对于他，布加拉提、米斯达、乔鲁诺等人应该都是这句话更好的践行者。其实这不过是老生常谈的宿命论，但如何看待，如何行动会收获不一样的心境和结果，就像茸茸说的那样，所谓觉悟，不是牺牲的心，而是在黑暗的荒野中开辟出一条前进的道路。我希望自己能成为一个有觉悟的人，不为自己看到的所谓“宿命”而哀叹或者是丧气的接受，而是不断追求。 ","date":"2021-08-11","objectID":"https://maxwellf1.github.io/2021-08-11/:2:1","series":null,"tags":[],"title":"写在开始的话","uri":"https://maxwellf1.github.io/2021-08-11/"},{"categories":[],"content":"2.2 关于一些事情 博客的title叫Separator，Wiki词条中对它有更详细的说明。大概是两年前(2019年底2020年初)的样子我给自己想到了这个NickName，自以为很妙，我很喜欢这个代号。一方面和我的名字有一定的联系，另一个方面，它也说明了我的转变，也是我努力的方向。 应该是那年某学期选课的时候，和本科导师聊过几句，每次见面的时候他都是鼓励我，那次也不例外，但有些话大概是作为第一个分隔符的存在，让我的心态发生了转变。那次突然想和老师聊聊未来的规划，就不怕丢人的说了一些自己的情况和想法，也斗胆问了一下老师当年的故事。他居然很乐意和我聊这些，先肯定了我的一些想法，也告诉我他自己当年也有过迷茫的时候，但一个人要做的并不是封闭自我或者自暴自弃，而是要迫使自己接触更多的东西，去做更多的尝试，同时也更努力的去做一些目前应该做好的事儿，他说只要这样，就算我考试60分甚至不及格，在他心里都是一个很优秀的本科生。我当时的成绩是在系里很下游，已经太久没有跟“优秀”沾过边了，老师说的话刺痛了我也点醒了我，我之前所做的那些不过是逃避和借口罢了。 从那学期开始，我决心多学多看多钻研，也开始做那些之前觉得笨学生才会做的事情，因为我明白，自己已经是一个又笨基础又差的学生了。应该是在那个学期，我拿到了大学以来第一个专业课90+，从此之后，第二个、第三个…, 92、95、99…，那几个学期九十左右的均分也让我最终奇妙的获得了保研资格。还主动进组体验了几次科研，并且发表了一篇短文，虽然只是在一个规模很小的Workshop上，但也算长了长见识。这里，又是一个分隔符，我不再看不起这些那些，开始鼓励自己多去尝试，“不试试怎么知道行不行呢”，“人总要有梦想的，万一实现了呢”。 后来发生了更多奇妙的事情，也遇到了很多贵人帮助，让我划上了一个又一个的分隔符，心理和状态不断进入新的阶段。非常感谢这些人这些事儿❤️。我也深知，在以后的日子里，大概会划上更多新的分隔符，但是新阶段的情况和走向如何，才是我应该考虑并为之努力的。此外，Separator另一个意思为分离的装置，希望在以后的日子里，我能将知识、项目、乃至生活等等都做出一个好的分离，清晰透彻的观察问题，并将各组分做良好的拆分，安排好、解决好各种事务。 ","date":"2021-08-11","objectID":"https://maxwellf1.github.io/2021-08-11/:2:2","series":null,"tags":[],"title":"写在开始的话","uri":"https://maxwellf1.github.io/2021-08-11/"}]