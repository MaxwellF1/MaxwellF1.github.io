# 3Blue1Brown-Neural Networks


## Chapter 1-But what is a Neural Network

以识别数字的例子进行引入，人脑可以将看到的不同形状写法的数字分辨成0-9的某一个，但是如何让机器程序来解决？

将这个**问题**描述为：编写一个程序，输入是一个28×28像素的网格，输出0到9之间的某个数字。

这个对于人眼和大脑都非常容易的问题，对于程序却不好描述，比如传统的if语句、类、对象等等。神经网络则试图编写一个模仿大脑的程序来解决这些问题。

### 神经网络的结构

神经网络多种多样，例如图像处理中广泛使用的CNN，还有RNN和transformer等。我们从最简单最经典的模型来考虑，即多层感知机Plain vanilla(aka "multilayer perceptron")

![img](../../../images/2021/08/plain-vanilla.png)

#### 神经元

首先将神经元考虑为一个“装有数字的容器”，0.0-1.0之间的值表示被激活的“程度”，来类比人脑中神经元的活跃程度。一堆连接在一起的神经元就构成了神经网络。

对于本问题，输入图像中有28 × 28 = 784个像素，每一个的亮度值都介于0.0和1.0之间，因此我们在网络中对应设计一个有784个神经元的 **“输入层”** ，同样的最后一层的 **“输出层”** 包含10个神经元，代表0-9中某个可能的数字。从图中可以看出，在中间还会有一些层，即 **“隐含层”** ，涉及到一些处理，在图中的网络中设计了2个隐含层，每一层有16个神经元，设置可以是任意的，可以根据情况选择层数和神经元数。

#### 层(Layers)

为什么需要Layers？可以从图中看到某一层的每个神经元会通过一条线连接到下一层的每一个神经元，通常这些线是带有权重的，表示某层中每一个神经元的"activation"(激活)如何对下一层的每个神经元的激活产生影响。而确定这些权重的选择，是神经网络作为信息处理机制。我们需要考虑几个问题：为什么期望这样的分层结构能够智能运行？我们期待什么？中间层做的是什么？为什么不直接把所有像素连接到最终输出？

先从一个 **不一定正确** 的想法谈起，我们可以希望倒数第二层的每个神经元识别出组成数字的更小的、可识别的组件，比如圆圈，长线段等等，并希望第二层可以对应一些更小的组分，如下图所示：

![img](../../../images/2021/08/layer-hypothesis.png)

虽然事实可能与我们这种猜想不同，但我们可以推断到的是，层将问题分解为一些很小的“碎片”。除了图像处理外，解析语音也会用到分层，例如先讲原始音频解析为不同的声音，然后组合成某些音节，再结合成单词，进一步构成短语以及更想法等。因此，神经网络的分层结构是很棒的，它允许我们将困难的问题分为bite-size steps，因此从一层移动到下一层相对会简单一些。

### 信息在层之间的传递

在神经元之间的连线上分配权重，代表着该层的神经元与下一层的新神经元如何相关。如果该层的某个神经元是激活的，正权重代表着与之相连的下一层神经元也应该是激活的，负权重则相反。各个权重会以一种有趣的方式相互作用和冲突。得到第二层的神经元的值需要计算第一层所有神经元激活值的加权和。我们自然可以想到如果我们想找某一块中是否有某一个小“碎片”，不妨将目标区域对应的权重都设为正值，其它赋值为零，这样的确在目标区域有我们希望的“碎片”时更容易将下一层的神经元激活，**但是**设想一种输入像素全都是激活(纯白)的图像，这种方法的计算结果仍然是“激活”，所以我们如果真的想确定这存在着一条短边(“碎片”)，需要给目标区域周围一圈的权重赋值为负，这样当目标区域像素亮而周围像素暗的时候，综合将最大，例如对于下图的例子中，激活程度B>D>A>C.

![img](../../../images/2021/08/weights-question.png)

#### Sigmoid Squishification & Bias

上述加权和可能是任何一个数，但是对于该网络我们希望激活值处于0.0-1.0之间，所以需要对得到的结果进行“压缩”。sigmoid是其中一个常用的函数。

![img](../../../images/2021/08/sigmoid.png)

但我们希望的可能并非是在大于零时将神经元激活，可能希望加权和大于某个设定值再让其激活，所以我们可以引入Bias，计算公式为：
$$
\sigma(\omega_{1}a_{1} + \omega_{2}a_{2} + ... +\omega_{n}a_{n} + Bias)
$$
可以得出，权重告诉我们第二层的神经元关注什么样的pixel pattern，而Bias告诉我们加权和需要多大才认为神经元的激活有意义。

**注**：sigmoid是有很明显的缺点的。从图中可以看出在数变得极大或者极小时这个函数的变化非常平坦，这不利于我们训练网络。因为当我们调整权重的时候，可能根本没有明显的变化。所以当前广泛采用ReLU(Rectified Linear Unit)。它的输出永远不会变平缓，因此调整权重总能够提供有效的反馈，使得训练过程更快更高效，尤其是设计的层数很多时(深度学习)。

我们采用sigmoid/ReLUd的原因是如果我们单纯的计算加权和，整个函数会是**线性**的，这会使得网络dramatically less expressive。

#### More neorons

刚刚是针对一个神经元讨论，但第一层就有784个神经元，则与第二层之间有784×16个权重和16个Bias，加上其它层，权重和偏差加起来一共有13002个，也就是说我们需要针对很大规模的参数进行调节。在之前提到我们对于网络工作模式的设想不一定是正确的，但比起将隐含层当成一个黑箱，这样通过设想来猜测意义，可以对我们修正结构有方向性的帮助。理想情况下网络既能够正确工作，我们也能知道结构设计和参数的缘由。可以通过一些简化的记号通过矩阵的写法来表示众多的参数，这也利于编程：

![image-20210815001821577](../../../images/2021/08/compact_notation.png)

### 网络只是一个函数

之前提到将神经元看做保存数字的容器，其实将其视为一个函数更加准确，它的输入是上一层神经元的所有输出，它的输出是一个0-1之间的值，而图示的整个网络，可以看做输入784的值，输出10个值的函数。



## Chapter2 Gradient descent, how neural networks learn

梯度下降法是训练的核心思想，不仅是神经网络学习的基础，也是其它许多机器学习技术的基础。

### 网络如何学习

机器学习与其它计算机科学的不同之处在于，我们没有为执行特定任务编写指令。对于第一节的例子，我们没有编写识别数字的算法，相反我们编写了一个可以接收一堆手写数字的示例图像以及它们是什么的标签的算法，并且调整网络的13002个参数(输入的标记图像称为“训练数据”)。我们通过这些数据来训练网络，然后使用一些网络没见过的数据在训练后对其进行测试。

机器的学习过程更像是一个微积分的问题，它目的是找到特定函数的最小值。

![img](../../../images/2021/08/finding-minima.png)

#### 成本函数

现在很多时候将cost/lost function混为一谈。回顾一下，加权值和中的权重可以认为代表连接的强弱，而偏置值则表明神经押元是否更容易被激活。首先将所有权重和偏差初始化为随机数，网络会表现很糟，需要设计一种方式来识别目前做的很糟糕，然后帮助它改进。所以我们来定义一个**成本函数**，将输出与期望值之间的差异平方相加，称为该训练数据的“成本”。

![img](../../../images/2021/08/cost-calculation.png)

#### The Cost Over Many Examples

为了真正衡量网络的性能，我们要考虑所有(可能数万个)训练示例的**平均**成本，这个平均值是我们衡量网络有多糟糕以及计算机应该感觉有多糟糕的指标。同样的，这个平均成本也是一个复杂的函数，网络本身就是一个复杂的函数，平均成本(代价函数)要再网络的基础上进行更高一层的计算，它将13002个权重和偏置值作为其参数，而输出一个数值(差的平方和)。

#### Minimizing the cost function

告诉计算机有多么糟糕只是一部分，我们更需要告诉它应该如何改进。对于简单的函数，我们可以通过求导，来知道在什么位置可以取到(局部)最小值，但对于复杂的函数，本次实例中上万个参数的函数，是无法这样进行的。一个更灵活的技巧是，先随便挑一个输入值，然后考虑往什么方向走，可以将函数值减小，这样在每一个点都求梯度，并沿着下降的方向走一小步(斜率越小这个步长就越小，从而防止调整过度)，就可以逼近函数的某个局部最小值(可能存在很多，取决于从哪个随机输入开始)，可以想象成小球从山上往下滚。

#### 梯度

在高维空间中我们需要一个向量来表示最陡峭的方向，多元微积分中将这个向量称为”梯度“，代表哪个方向增加函数最快，取该向量的反方向就可以得到最快减小函数的步进方向，这个梯度向量的长度表示了这个最陡的斜坡有多陡。所以最小化这个函数的算法是计算这个梯度方向，然后下降，不断重复，这个过程就叫做”梯度下降“。

如果我们把这13002个权重和偏置值都放到一个列向量里，则代价函数的(负)梯度也会是一个有13002个条目的向量。

![img](../../../images/2021/08/weights-and-gradient.png)

负梯度指出了我们应该如何调整每个参数，才可以让代价函数的值下降的最快。该向量的每个分量告诉我们两件事，符号表示相应分量应该向上还是向下，更重要的一点是，梯度中各个分量的相对大小告诉我们这些调整哪一个更为重要(改变哪个性价比更高)。计算梯度的算法是神经网络的核心，称为back propagation(反向传播算法)。

![img](../../../images/2021/08/gradient-explained.png)

注意，我们一直提到寻找最小值，这要求代价函数必须是平滑的，这也解释了为什么人工神经元的激活值是连续的，而不是二元式。

回顾一下我们之前设想的，第二层识别一些小的“碎片”，第三层识别更大一些的pattern，但事实上这个网络本身并不是这样做的，第二层的结果看起来会杂乱无章，只是代表某个局部最小值而已。而且因为我们的训练数据只有从0-9的标签数据，相当于整个空间只局限于此，所以训练好的网络也可能将某个毫无意义的噪声图像识别为某个数字。当然MLP只是一个很古老的模型，现在有更多更好用的新模型出现。

## Chapter 3-Analyzing our neural network

这是一个mini lesson。将分析训练好的网络做了什么以及原因。之前例子中的网络训练好后大概有96%的正确率，这其实是难以置信的，因为我们从来没有明确告诉它要寻找怎么样的pattern。

之前我们猜测第二层识别小的边缘，第三层识别更大一些的子组件来激发这个结构。但事实上我们的网络不是这样的，查看从第一层到下一层的过度相关联的权重时，它们看起来几乎是随机的，虽然有一些松散的pattern，但显然不是我们之前设想的：

![img](../../../images/2021/08/second-layer-weights.png)

似乎在可能权重和偏差的13002维空间中，网络发现了某一个局部最小值，尽管成功的对大多数图像进行了分类，但并没有完全掌握更普遍的模式，例如之前提到的随机输入一个噪声图像，智能的系统应该输出不确定的结果(要么没有激活，要么均匀激活输出层)，但其实它自信的给出了一些无意义的答案。而且这个网络的训练数据基本都是居中且大小合适的数字，并没有让网络在网格的一个区域上获取的模式知识转移到另一个区域，我们的训练算法甚至没有使用某些像素和其他像素相邻的事实。

## Chapter4-What is backpropagation really doing

### The Intuition for Backpropagation

反向传播中的各种符号和上下标一开始会使人困扰。首先我们忽略这些公式符号，从逐步了解每个训练示例对权重和偏差的影响开始。

![img](../../../images/2021/08/classify-as-2.png)

对于这个样本，我们希望网络分类为2，我们希望输出值的变动大小应该与当前值和目标值之间的差异成正比，例如，增加数字“2”神经元的激活值要比减少数字“8”的激活值更为重要，因为后者已经接近理想输出。我们以“2”神经元为例：

![img](../../../images/2021/08//output-neuron-equation.png)

它的激活值是来自于前一层所有的加权综合，加上一个偏置值，最后经过sigmoid/ReLU等进行处理。因此我们有三种方法来增大这个激活值：

1. 增加偏置值
2. 增加权重
3. 改变上一层的激活值

#### 增加偏置值

这是最简单的方法，与改变前一层的权重或者激活值不同，改变偏置对加权和的影响是恒定而且可预测的。

#### 增加权重

权重与激活值相乘，所以具有不同程度的影响，与来自前一层更亮的神经元的连接具有较大的影响，因为这些权重与更大的激活值相乘。即当我们进行梯度下降时，不仅关心每个是要向上还是向下调整，更关心调整哪些参数性价比最高。

这里类似于神经科学中关于神经元如何学习的理论——“赫布理论(Hebbian theory)”，即“一同激活的神经元关联在一起”。这里，权重的最大增长，即最大的连接加强的部分，就会发生在已经最活跃的神经元和我们希望变得更活跃的神经元之间。

#### 改变前一层的激活值

如果前一层中所有正权重连接的神经元更亮而负权重连接的神经元更暗的话，就可以增加我们想要的神经元的激发程度。与改变权重的时候类似，我们想造成更大的影响，就要根据对应权重的大小，对激活值做出成比例的改变。当然，我们无法直接改变激活值。

### 对所有的训练数据重复

我们之前只考虑了要增强2的激活，但还要减弱最后一层中其他神经元的激活。但每个神经元对于如何改变倒数第二层都有自己的想法，这意味着有许多竞争性的请求来改变前一层。所以我们会将数字“2”神经元的期待和其它输出神经元的期待全加在一起，作为如何改变倒数第二层神经元的指示。这些期待的变化不仅与对应的权重成比例，也与每个神经元激活值需要改变的量成比例。这其实就是“反向传播”的理念，我们可以一直递归的将这个过程进行从后一层到前一层，直到第一层。

刚刚我们讨论的是仅仅对一个训练样本进行的过程，我们还需要对其他所有的训练样本同样的进行一遍反向传播。记录下每个样本想要怎样修改，然后取平均值。

![average-over-all](../../../images/2021/08/average-over-all.png)

这里一系列的对每个权重和偏置的平均微调大小，不严格的讲，就是之前说的代价函数的负梯度(至少是其标量的倍数)。

### Stochastic Gradient Descent

实际操作中如果梯度下降的每一步都用上每一个训练样本来计算会非常耗时。可以采取一个技巧，首先将训练数据打乱并分成很多组minibatch。然后根据每个minibatch而不是整个训练样本集来计算梯度下降。它并不会提供准确最有效的梯度下降，但给出了一个很好的近似值，而且计算时间大大减少，这就是随机梯度下降。我们并不是小心谨慎的一步步沿着最佳方向走到底部，而是有点像个醉汉快速但是绕着小圈子跑下去。

![sgd](../../../images/2021/08/stochastic-comparison.png)

注：我们需要非常多的标记好的数据才能良好的完成训练过程。

## Chapter5-Backpropagation calculus

从微积分的角度看在机器学习中我们一般是怎么理解链式法则的。

梯度向量的条目是代价函数相对于网络所有不同权重的偏置的偏导数，也就是说反向传播帮我们计算出了那些导数。

### 用反向传播计算梯度

首先我们设定一些记号：

![img](../../../images/2021/08/backpropagation_notations.png)

a<sup>(L)</sup> 表示L层某个神经元的激活值，y是我们期待的输出，C<sub>0</sub>是代价函数，我们想知道在数轴上挪动ω可以对C造成多大影响，就是求导，可以利用链式法则。

![img](../../../images/2021/08/derivation_all_examples.png)

网络的完整代价函数是求的每个样本的平均值，所以导数我们也要求平均值。而这个只是梯度向量的一个分量，所有权重和偏置的导数组成梯度向量。从求导结果我们可以看出，当前一个神经元更活跃时(a<sup>(L-1)</sup>)，改变ω可以有更好的效果，偏导的最后一部分告诉我们该导数与实际输出与期望输出之间的差异成正比，也就是说当实际输出偏差很大时，即使微小的改动也会对代价产生明显的影响。

偏置的梯度更简单一些：

![img](../../../images/2021/08/derivation_bias.png)

### 前一层的权重和偏置

我们已经确定了最后一层的权重和偏置的变化如何影响总体的成本，这意味着我们已经有了梯度向量的两个部分(前面提到三种改变方式)。现在我们要探讨一下代价函数对上一层激活值的敏感程度(对a<sup>(L-1)</sup>求偏导)。虽然我们不能直接改变激活值，但我们仍应该关注这一方面，因为我们可以反向利用链式法则，来计算代价函数对之前(前一层)的权重和偏置的敏感度。

![img](../../../images/2021/08/previous.png)

例如求偏导$\frac{\partial(C_0)}{\partial(w^{(L-1)})}$​​​可以通过如下的链式法则得出，因此我们可以通过链式法则得到任何一个参数的导数，即可以计算出整个梯度向量的各个分量。
$$
\frac{\partial(C_0)}{\partial(w^{(L-1)})}=\frac{\partial(z^{(L-1)})}{\partial(w^{(L-1)})}\frac{\partial(a^{(L-1)})}{\partial(z^{(L-1)})}\frac{\partial(z^{L})}{\partial(a^{(L-1)})}\frac{\partial(a^{L})}{\partial(z^{L})}\frac{\partial(C_0)}{\partial(a^{L})}
$$

### 更复杂的网络

上面的例子中是“一条线”的简单网络，对于复杂网络其实没有太大变化，只是多了一些下标而已，来指出这是该层的哪个神经元对应的量。而代价函数和对于上一层激活值的偏导变成了求和的模式：

![img](../../../images/2021/08/backpropagation_general.png)

可以用下面的公式总结，对于下面黄色框的公式，当不是最后一层的时候用上式，是最后一层用下式。

![img](../../../images/2021/08/backpropagation_conclusion.png)


